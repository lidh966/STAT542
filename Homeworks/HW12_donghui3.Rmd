---
title: 'STAT 542: Homework 12'
author: "FALL 2020, by Ruoqing Zhu (rqzhu)"
date: 'Due: Monday, Nov 30, 11:59 PM CT'
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
---

```{r setup, include=FALSE}
  knitr::opts_chunk$set(include = TRUE)  # TRUE for solution; FALSE for questions set
  knitr::opts_chunk$set(echo = TRUE)
  knitr::opts_chunk$set(message = FALSE)
  knitr::opts_chunk$set(warning = FALSE)
  knitr::opts_chunk$set(fig.height = 6, fig.width = 8, out.width = '80%', fig.align = "center")
  options(width = 90)
```

```{css, echo=FALSE}
.solution {
background-color: #e6ffe6;
}
```

## Instruction

Students are encouraged to work together on homework. However, sharing, copying, or providing any part of a homework solution or code is an infraction of the University's rules on Academic Integrity. Any violation will be punished as severely as possible. Final submissions must be uploaded to compass2g. No email or hardcopy will be accepted. For [**late submission policy and grading rubrics**](https://teazrq.github.io/stat542/homework.html), please refer to the course website.

- You are required to submit two files:
    - Your `.rmd` RMarkdown (or Python) file, which should be saved as `HWx_yourNetID.Rmd`. For example, `HW1_rqzhu.Rmd`.
    - The result of knitting your RMarkdown file as `HW1_yourNetID.pdf`. For example, `HW1_rqzhu.pdf`. Please note that this must be a `.pdf` file. `.html` format cannot be accepted.

- Include your Name and NetID in your report.
- If you use this file or the example homework `.Rmd` file as a template, be sure to remove this instruction section.
- Your `.Rmd` file should be written such that, if it is placed in a folder with any data you utilize, it will knit properly without modification.
- Make sure that you set seed properly so that the results can be replicated. 
- For some questions, there will be restrictions on what packages you can use. Please read the requirements carefully. 

## About HW12

You should implement a stump model which consists of just one split. Use the stump model as the base learner in AdaBoost, following the algorithm we introduced.

## Question 1 [100 Points] AdaBoost with stump base learner

Let's write our own code for a one-dimensional AdaBoost using a tree stump model as the base learner. 

* The stump model is a CART model with just one split, hence two terminal nodes. Since we consider just one predictor, the only thing that needs to be searched in this tree model is the cutting point. Write a function to fit the stump model with subject weights:
    + __Input__: A set of data ${\cal D}_n =  \{x_i, y_i, w_i\}_{i=1}^n$
    + __Output__: The cutting point $c$, and node predictions $f_L, f_R \in \{-1, 1\}$
    + __Step 1__: Search for a splitting rule $\mathbf{1}(x \leq c)$ that will maximize the weighted reduction of Gini impurity.
$$ \texttt{score} = - \, \frac{\sum_{ {\cal T}_L} w_i}{\sum w_i} \text{Gini}({\cal T}_L) - \frac{\sum_{ {\cal T}_R} w_i}{\sum w_i} \text{Gini}( {\cal T}_R ),$$ where, for given data in a potential node ${\cal T}$, the weighted version of Gini is
$$ \text{Gini}({\cal T}) = \widehat p (1- \widehat p), \qquad \widehat p = (\textstyle \sum w_i)^{-1} \textstyle\sum w_i I(y_i = 1).$$
    + __Step 2__: Calculate the left and the right node predictions $f_L, f_R \in \{-1, 1\}$ respectively.

* Based on the AdaBoost algorithm, write your own code to fit the classification model, and perform the following
    + You are required to implement a `shrinkage` factor $\delta$, which is commonly used in boosting algorithms.
    + You are not required to do bootstrapping for each tree (you still can if you want).
    + You should generate the following data to test your code and demonstrate that it is correct.
    + Plot the exponential loss $n^{-1} \sum_{i=1}\exp\{- y_i \delta \sum_k \alpha_k f_k(x_i)\}$ over the number of trees and comment on your findings.
    + Try a few different `shrinkage` factors and comment on your findings. 
    + Plot the final model (functional value of $F$, and also the sign) with the observed data.

```{r}
  set.seed(1)
  n = 300
  x = runif(n)
  py <- function(x) sin(4*pi*x)/3 + 0.5
  y = (rbinom(n, 1, py(x))-0.5)*2
  plot(x, y + 0.1*runif(n, -1, 1), ylim = c(-1.1, 1.1), pch = 19, 
       col = ifelse(y == 1, "darkorange", "deepskyblue"), ylab = "y")
  lines(sort(x), py(x)[order(x)] - 0.5)
  
  testx = seq(0, 1, length.out = 1000)
  testy = (rbinom(1000, 1, py(testx))-0.5)*2
```
### Answer

```{r}

# calculate for ginfunction
gini_index <- function(sum_wy, sum_w) {
  p = sum_wy / sum_w
  return(p * (1-p))
}

# function for CART model

CART <- function(x,y,w){
  n <- length(x)
  
  # initial for score calculation
  left_sum_w = 0
  right_sum_w = sum(w)
  left_sum_wy = 0
  
  # in order to calculate gini index
  right_sum_wy = sum(w*(y==1))
  
  # find all potential x
  x_order = order(x)
  scores = c()
  
  for(k in c(1:n)){
    tem_order = x_order[k]
    tem_wy = (y[tem_order]==1) * w[tem_order]
    left_sum_w = left_sum_w + w[tem_order]
    right_sum_w = right_sum_w - w[tem_order]
    left_sum_wy = left_sum_wy + tem_wy
    right_sum_wy = right_sum_wy - tem_wy
    
    scores[k] = -(left_sum_w * gini_index(left_sum_wy,left_sum_w) +
                    right_sum_w * gini_index(right_sum_wy,right_sum_w)) / sum(w)
  }
  
  max_k = which.max(scores)
  max_x_index = x_order[max_k]
  c = x[max_x_index]
  left = sign(sum(y[x_order[1:max_k]]*w[x_order[1:max_k]]))
  if(left==0){
    left=1
  }
  right = sign(sum(y[x_order[(max_k+1):n]]*w[x_order[(max_k+1):n]]))
  #print(right)
  if(right==0){
    right=1
  }
  # return the order of x 
  return(c(max_k, c, left, right))
}

classify <- function(A,x){
  T = nrow(A)
  pred_y = x*0
  for(i in 1:T){
    tem_pre = pred_y*0
    tem_pre[x<=A[i,2]]=A[i,3]
    tem_pre[x>A[i,2]]=A[i,4]
    pred_y = pred_y + tem_pre*A[i,5]
  }
  
  pred_y = sign(pred_y)
  pred_y[pred_y==0] = 1
  return(pred_y)
}

adaBoost <- function(x,y,T,delta,x_test,y_test){
  n = length(y)
  x_order = order(x)
  # initialization of w
  w = rep(1/n,n)
  loss = y * 0
  loss_test = y_test * 0
  resutls = matrix(data=NA,nrow=T,ncol=5)
  e_loss = c()
  e_loss_test = c()
  for(i in 1:T){
    resutls[i,1:4] = CART(x,y,w)
    pre_y = y*0
    pre_y[x_order[1:resutls[i,1]-1]] = resutls[i,3]
    pre_y[x_order[(resutls[i,1]):n]] = resutls[i,4]
    
    pre_y_inv = -pre_y
    tem_e = sum(w*(pre_y_inv==y))
    #print(sum(w * (pre_y != y)))
    a = 1/2 * log((1-tem_e)/tem_e)
    resutls[i,5] = a
    w_new = w * exp(-a * y * delta * pre_y)
    w = w_new / sum(w_new)
    loss = loss - y * delta * (a * pre_y)
    e_loss[i] = mean(exp(loss))
    
    pre_y_test = y_test*0
    
    pre_y_test[x_test<resutls[i,2]] = resutls[i,3]
    pre_y_test[x_test>=resutls[i,2]] = resutls[i,4]
    loss_test = loss_test - y_test * delta * (a * pre_y_test)
    e_loss_test[i] = mean(exp(loss_test))
    
    #print(c(mean(exp(loss_test)),mean(exp(loss))))
    
  }
  #print(length(x_test))
  out <- list(
    'e_loss'=e_loss,
    'trees'=resutls,
    'e_loss_test'=e_loss_test
  )
  return(out)
}

# 

deltas = c(0.1,0.2,0.5,0.7,0.9,1)

errs = NULL
e_err = NULL
e_err_test = NULL
#e_err_train = test
for(k in 1:length(deltas)) {
  #print(deltas[k])
  model = adaBoost(x, y, 300, deltas[k], testx, testy)
  pred = classify(model$trees, x)
  trainErr = sum(pred != y) / length(y) * 100
  pred = classify(model$trees, testx)
  testErr = sum(pred != testy) / length(testy) * 100
  #print(c(testErr,trainErr))
  errs = cbind(errs, c(trainErr, testErr))
  e_err = cbind(e_err, model$e_loss)
  e_err_test = cbind(e_err_test, model$e_loss_test)
}

```

```{r}
par(bg = 'grey')
matplot(e_err[,1:6],type='l', lty = "solid", ylim=c(0.85, 1.1), col=1:6,
        pch=NULL,main="Exp error vs iterations, shrinkage", xlab="Iterations", 
        ylab="ExpErr")
legend(x="topleft", lty=1, legend=deltas[1:6], col=1:6, title="training")

matlines(e_err_test[,1:6], type="l",lty = 2, ylim=c(0.85, 1.1), col=1:6)
legend(x="topright", lty=2, legend=deltas[1:6], col=1:6, title="testing")
```

```{r}
final_model = adaBoost(x, y, 50, 0.7, testx, testy)
fit_resutls = final_model$trees
F_test = testy*0
F_train = y*0

for(i in 1:T){
  tem_test = F_test*0
  tem_train = F_train*0
  
  tem_test[testx<fit_resutls[i,2]] = fit_resutls[i,3]
  tem_test[testx>=fit_resutls[i,2]] = fit_resutls[i,4]
  
  tem_train[x<fit_resutls[i,2]] = fit_resutls[i,3]
  tem_train[x>=fit_resutls[i,2]] = fit_resutls[i,4]
  
  F_test = F_test + tem_test*fit_resutls[i,5]
  F_train = F_train + tem_train*fit_resutls[i,5]
}
# training accuracy
mean(sign(F_train)==y)
plot(x, y + 0.1*runif(n, -1, 1), ylim=c(-1.1, 1.1), pch=19,
     col=ifelse(sign(F_train)==1, "darkorange", "deepskyblue"),
     ylab="y", main="Training classification")
lines(sort(x), py(x)[order(x)]-0.5)
lines(sort(x), F_train[order(x)], type="l", col=c("blueviolet"))
legend(x=0.85, y=0.85, legend=c(1, -1), col=c("darkorange", "deepskyblue"),
pch=19, title="classification")

```
```{r}
plot(testx, testy + 0.1*runif(1000, -1, 1), ylim=c(-1.1, 1.1), pch=19,
     col=ifelse(sign(F_test)==1, "darkorange", "deepskyblue"),
     ylab="y", main="testing classification")

# testing accuracy
mean(sign(F_test)==testy)
lines(sort(testx), py(testx)[order(testx)]-0.5)
lines(sort(testx), F_test[order(testx)], type="l", col=c("blueviolet"))
legend(x=0.85, y=0.85, legend=c(1, -1), col=c("darkorange", "deepskyblue"),
pch=19, title="classification")
```




